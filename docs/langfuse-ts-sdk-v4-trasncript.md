Langfuse TypeScript SDK v4 (GA): OTel-native tracing, improved DX, performance, and integrations. - YouTube
https://www.youtube.com/watch?v=BjEXs13KyV4

Transcript:
(00:01) Hi everyone, my name is Hib. I'm an engineer here at Langfuse. And today is a very exciting day for us because today we are launching the new JavaScript TypeScript SDK. And to be honest with you, it was about time because our Python users for a long time had the benefit of using the observe decorator.
(00:22) The observe decorator was a higher level abstraction that allowed them to trace any of their existing functions um by simply adding the observed decorator to it and they would have gotten duration tracking, input output tracking and automatic context management out of the box. This meant that if they would have opened a new span inside the function body of a decorated function, it would have been automatically nested correctly.
(00:49) Now, our JS and TS users had a tougher time to maintain context across the applications because they only had a lower level API available to them. They would have needed to open traces manually and use the returned clients in order to manage the nesting. So you see here that we creating a trace by calling length is a trace and then we need to call trace generation to nest the generation under the trace and if we would have nested another observation underneath it we would have needed to call the specific methods on that
(01:24) specific client. This meant that if nesting and instrumentation is spread across the application one had to pass around the reference to that object in order to manage the nesting correctly. We now have great news for our Typescript users. With the new TypeScript SDK V4, we have three higher level methods available in order to trace and instrument your application.
(01:48) The first one is the context manager. You can use start active observation in order to simply pass a callback to it and have automatic duration tracking and context management out of the box available. You can see here that we are starting active observation. We have wrapped around some kind of LM operation that we're doing here and within the callback we are updating the existing span with a span.
(02:17) date method and the span is being passed into the call back as an argument. And we could have also used the update active observation and update active trace methods in order to simply update the currently active trace observation. You can also use the observe wrapper. The observe wrapper is pretty much analog to the observed decorator that we have in Python.
(02:41) However, we're not using a decorator pattern here, but simply a wrapping of existing functions. Additional to tracking the duration of the function execution and having automatic context management, we are also tracing the inputs and outputs of that observed function. For more advanced use cases, you can also use the start observation method which gives you the most low-level control about the full span life cycle.
(03:11) Let me now show you how the JS SDK works in action. And for that I will show you how our demo project is looking like on our website. We have an interactive demo on our website and we have a chatbot here. And that chatbot runs Nex.js JS deployed on Vessel using the Vela SDK and we have now added instrumentation for that such that we're getting the scores we are getting the traces shown up in Langfuse.
(03:41) Let me show you first how the code behind that looks like. We are in our langu repository in QA chatbot API headlamp. We are exposing the post function which then the next.js JS framework is then exposing as an endpoint and we have declared an handler here that we have wrapped in the observe wrapper.
(04:08) We are passing the name option to it and the name should be handled chatbot message and we're configuring end on exit false such that we are manually ending the observation such that we can control that the observation is ended after the response has been streamed to the client and not when the stream begins. So let's take a look at how the handler looks like.
(04:32) And we see that inside the handler we are passing the messages and doing our basic request logic. But we can now also use our higher level functions exposed by the new types of SDK to update the current active observation and the current active trace. Furthermore on the top offer file we have also declared that we are observing the prompt fetching that we're doing.
(04:56) So, length also offers prompt management and for fetching the prompt we are also want to have span available in lenfuse and for that we are simply observing the lenfist client prompt get method let's do an example execution and see how the trace looks like in lenfuse so I will ask now how does the typescript start active observation function work.
(05:28) The agent now decides to do two calls to first search Lexus docs and get a Lexus overview before they synthesize the result. And we see we're getting a code snippet back. And the explanation is that it's a context manager style helper that automatically handles the life cycle of an observation and the open telemetry context underneath it. Great.
(05:52) Let's take a look now at the langu demo project whether we see a new trace coming in and indeed we see a new trace with the input how does the TypeScript start active observation function work which is exactly what I've just typed let's take a look at the trace and we see that we have our handle chatbot message span we have a span for fetching the languages prompt and we also see that for the native AI SDK open telemetry instrumentation we see the spans showing ing up here exactly at the right position as we have desired and
(06:26) also with the same richness of information as you would have expect. For example, for the last execution we are getting token information, cost information, latency information and also the input output that went into the LLM and was also returned from the LM. Let's wrap up here. So you see the new TypeScript SDK is a leap forward for users.
(06:53) They can now use higher level abstractions in order to instrument their LLM app. It could be a context manager, it could be the observe wrapper or it could be opening and closing manual observations to trace their LLM app. They can also use um native instrumentation such as from the visel SDK, the open SDK which is supported by lenfus directly or lang chain which is also supported by lenfus directly.
(07:18) We are very excited about this change and we can't wait to see what you're building out there. If you have any feedback, let us know on GitHub. And until then, thank you so much and see you soon.